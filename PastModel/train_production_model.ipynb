{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System Imbalance Prediction - Production Model Training\n",
    "\n",
    "This notebook trains the production ML models for predicting electricity system imbalance (MWh) for 15-minute settlement periods.\n",
    "\n",
    "## Model Overview\n",
    "- **Algorithm:** HistGradientBoostingRegressor (one model per lead time)\n",
    "- **Lead Times:** 15, 12, 9, 6, 3 minutes before settlement end\n",
    "- **Features:** Minimal set of 3-5 features per model (optimized for performance)\n",
    "- **Baseline:** IPESOFT weighted average formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Fix for Windows joblib issue\n",
    "import os\n",
    "os.environ['LOKY_MAX_CPU_COUNT'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_PATH = Path('../Data/live3minValue.csv')\n",
    "MODELS_PATH = Path('../models')\n",
    "MODELS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "# Lead times (minutes before settlement end)\n",
    "LEAD_TIMES = [15, 12, 9, 6, 3]\n",
    "\n",
    "# Optimized features per lead time (discovered through feature importance analysis)\n",
    "FEATURES = {\n",
    "    15: ['roll_mean_5', 'val_curr', 'roll_mean_10', 'roll_std_5'],\n",
    "    12: ['val_curr', 'roll_mean_5', 'lag_1', 'roll_mean_10', 'roll_mean_20'],\n",
    "    9:  ['val_curr', 'roll_mean_5', 'qh_cumsum', 'lag_2', 'roll_mean_10'],\n",
    "    6:  ['qh_cumsum', 'val_curr', 'roll_mean_5'],\n",
    "    3:  ['qh_cumsum', 'val_curr', 'roll_mean_5']\n",
    "}\n",
    "\n",
    "# Model hyperparameters\n",
    "MODEL_PARAMS = {\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 10,\n",
    "    'max_iter': 300,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Train/Test split ratio\n",
    "TRAIN_RATIO = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"Load and parse the live3minValue CSV.\"\"\"\n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        sep=';',\n",
    "        header=0,\n",
    "        names=['timestamp', 'value', 'extra'],\n",
    "        usecols=[0, 1],\n",
    "        dtype={'timestamp': str, 'value': str}\n",
    "    )\n",
    "        #Where value = 3MIN_RE_with_GCC\n",
    "\n",
    "    # Clean data\n",
    "    df = df[df['timestamp'].notna() & (df['timestamp'] != '')]\n",
    "    df = df[~df['timestamp'].str.contains('H.DaE', na=False)]\n",
    "    \n",
    "    # Parse timestamp and value\n",
    "    df['timestamp'] = df['timestamp'].str.replace('\"', '')\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], format='%m/%d/%Y %I:%M:%S %p.000', errors='coerce')\n",
    "    df['value'] = df['value'].str.replace('\"', '').astype(float)\n",
    "    \n",
    "    # Set index and resample to 3-min grid\n",
    "    df = df.dropna(subset=['timestamp'])\n",
    "    df = df.set_index('timestamp').sort_index()\n",
    "    df = df.resample('3min').mean()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "df = load_data(DATA_PATH)\n",
    "print(f\"Loaded {len(df):,} records\")\n",
    "print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
    "print(f\"Duration: {(df.index.max() - df.index.min()).days} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"Create features for the ML models.\"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Current value and lags\n",
    "    features['val_curr'] = df['value']\n",
    "    for i in range(1, 13):\n",
    "        features[f'lag_{i}'] = df['value'].shift(i)\n",
    "    \n",
    "    # Rolling statistics (5, 10, 20 steps = 15, 30, 60 min)\n",
    "    for window in [5, 10, 20]:\n",
    "        features[f'roll_mean_{window}'] = df['value'].rolling(window).mean()\n",
    "        features[f'roll_std_{window}'] = df['value'].rolling(window).std()\n",
    "    \n",
    "    # Intra-QH accumulation\n",
    "    features['qh_start'] = df.index.floor('15min')\n",
    "    features['qh_cumsum'] = features.groupby('qh_start')['val_curr'].cumsum()\n",
    "    features['qh_count'] = features.groupby('qh_start').cumcount() + 1\n",
    "    features = features.drop(columns=['qh_start'])\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Engineer features\n",
    "features = engineer_features(df)\n",
    "print(f\"Engineered {len(features.columns)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target(df):\n",
    "    \"\"\"Create target: systemImbalance (MWh) = mean of QH values × 0.25\"\"\"\n",
    "    target = df.groupby(df.index.floor('15min'))['value'].mean() * 0.25\n",
    "    target.name = 'target'\n",
    "    # Index by settlement end time (QH start + 15 min)\n",
    "    target.index = target.index + pd.Timedelta(minutes=15)\n",
    "    return target\n",
    "\n",
    "# Create target\n",
    "target = create_target(df)\n",
    "print(f\"Target samples: {len(target):,}\")\n",
    "print(f\"Target range: [{target.min():.2f}, {target.max():.2f}] MWh\")\n",
    "print(f\"Target mean: {target.mean():.2f} MWh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(features, target, lead_time, feature_cols):\n",
    "    \"\"\"\n",
    "    Create aligned dataset for a specific lead time.\n",
    "    \n",
    "    Timing: For prediction at (settlement_end - lead_time),\n",
    "    we use features from (settlement_end - lead_time - 3min) due to transmission lag.\n",
    "    \"\"\"\n",
    "    offset = pd.Timedelta(minutes=lead_time + 3)  # +3 for transmission lag\n",
    "    aligned_data = []\n",
    "    \n",
    "    for t_target, y in target.items():\n",
    "        t_cutoff = t_target - offset\n",
    "        if t_cutoff in features.index:\n",
    "            row = features.loc[t_cutoff, feature_cols].copy()\n",
    "            row['target'] = y\n",
    "            row['settlement_end'] = t_target\n",
    "            aligned_data.append(row)\n",
    "    \n",
    "    dataset = pd.DataFrame(aligned_data)\n",
    "    dataset = dataset.dropna()\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Create datasets for each lead time\n",
    "datasets = {}\n",
    "for lt in LEAD_TIMES:\n",
    "    datasets[lt] = create_dataset(features, target, lt, FEATURES[lt])\n",
    "    print(f\"LT={lt:2d} min: {len(datasets[lt]):,} samples, {len(FEATURES[lt])} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Baseline Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_predict(features_df, lead_time):\n",
    "    \"\"\"\n",
    "    IPESOFT Baseline predictor (weighted average formulas).\n",
    "    Output is in MWh.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for idx, row in features_df.iterrows():\n",
    "        lag1 = row.get('lag_1', row.get('val_curr', np.nan))\n",
    "        lag2 = row.get('lag_2', np.nan)\n",
    "        lag3 = row.get('lag_3', np.nan)\n",
    "        lag4 = row.get('lag_4', np.nan)\n",
    "        \n",
    "        if lead_time == 15:\n",
    "            pred = 0.25 * lag1\n",
    "        elif lead_time == 12:\n",
    "            pred = 0.25 * lag1\n",
    "        elif lead_time == 9:\n",
    "            pred = 0.25 * (0.8 * lag1 + 0.2 * lag2)\n",
    "        elif lead_time == 6:\n",
    "            pred = 0.25 * (0.6 * lag1 + 0.2 * lag2 + 0.2 * lag3)\n",
    "        elif lead_time == 3:\n",
    "            pred = 0.25 * (0.4 * lag1 + 0.2 * lag2 + 0.2 * lag3 + 0.2 * lag4)\n",
    "        else:\n",
    "            pred = np.nan\n",
    "        \n",
    "        predictions.append(pred)\n",
    "    \n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for results\n",
    "models = {}\n",
    "results = {}\n",
    "predictions_store = {'ml': {}, 'baseline': {}}\n",
    "\n",
    "print(\"Training models...\\n\")\n",
    "print(f\"{'LT':<8} {'Features':<10} {'Train':<10} {'Test':<10}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for lt in LEAD_TIMES:\n",
    "    dataset = datasets[lt]\n",
    "    feature_cols = FEATURES[lt]\n",
    "    \n",
    "    # Train/Test split (chronological)\n",
    "    split_idx = int(len(dataset) * TRAIN_RATIO)\n",
    "    train = dataset.iloc[:split_idx]\n",
    "    test = dataset.iloc[split_idx:]\n",
    "    \n",
    "    X_train = train[feature_cols]\n",
    "    y_train = train['target']\n",
    "    X_test = test[feature_cols]\n",
    "    y_test = test['target']\n",
    "    \n",
    "    print(f\"LT={lt:2d}    {len(feature_cols):<10} {len(train):<10} {len(test):<10}\")\n",
    "    \n",
    "    # Train model\n",
    "    model = HistGradientBoostingRegressor(**MODEL_PARAMS)\n",
    "    model.fit(X_train, y_train)\n",
    "    models[lt] = model\n",
    "    \n",
    "    # Predictions\n",
    "    ml_pred = model.predict(X_test)\n",
    "    \n",
    "    # For baseline, we need lag features\n",
    "    baseline_features = engineer_features(df)\n",
    "    baseline_test = create_dataset(baseline_features, target, lt, \n",
    "                                   ['val_curr', 'lag_1', 'lag_2', 'lag_3', 'lag_4'])\n",
    "    baseline_test = baseline_test[baseline_test['settlement_end'].isin(test['settlement_end'])]\n",
    "    bl_pred = baseline_predict(baseline_test, lt)\n",
    "    \n",
    "    # Store predictions with settlement times\n",
    "    for i, st in enumerate(test['settlement_end']):\n",
    "        if st not in predictions_store['ml']:\n",
    "            predictions_store['ml'][st] = []\n",
    "            predictions_store['baseline'][st] = []\n",
    "        predictions_store['ml'][st].append((y_test.iloc[i], ml_pred[i], lt))\n",
    "        if i < len(bl_pred):\n",
    "            predictions_store['baseline'][st].append((y_test.iloc[i], bl_pred[i], lt))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results[lt] = {\n",
    "        'ml_mae': mean_absolute_error(y_test, ml_pred),\n",
    "        'ml_rmse': np.sqrt(mean_squared_error(y_test, ml_pred)),\n",
    "        'ml_r2': r2_score(y_test, ml_pred),\n",
    "        'bl_mae': mean_absolute_error(y_test.iloc[:len(bl_pred)], bl_pred),\n",
    "        'ml_sign_acc': 100 * np.mean((y_test.values >= 0) == (ml_pred >= 0)),\n",
    "        'bl_sign_acc': 100 * np.mean((y_test.values[:len(bl_pred)] >= 0) == (bl_pred >= 0))\n",
    "    }\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CLASSICAL METRICS (Test Set)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'LT':<10} {'ML MAE':>10} {'BL MAE':>10} {'Improvement':>12} {'ML RMSE':>10} {'ML R²':>10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for lt in LEAD_TIMES:\n",
    "    r = results[lt]\n",
    "    imp = 100 * (r['bl_mae'] - r['ml_mae']) / r['bl_mae']\n",
    "    print(f\"LT={lt:2d} min  {r['ml_mae']:>10.3f} {r['bl_mae']:>10.3f} {imp:>+11.1f}% {r['ml_rmse']:>10.3f} {r['ml_r2']:>10.3f}\")\n",
    "\n",
    "# Averages\n",
    "avg_ml_mae = np.mean([results[lt]['ml_mae'] for lt in LEAD_TIMES])\n",
    "avg_bl_mae = np.mean([results[lt]['bl_mae'] for lt in LEAD_TIMES])\n",
    "avg_imp = 100 * (avg_bl_mae - avg_ml_mae) / avg_bl_mae\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Average':<10} {avg_ml_mae:>10.3f} {avg_bl_mae:>10.3f} {avg_imp:>+11.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SIGN ACCURACY (Test Set)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'LT':<10} {'ML SignAcc':>12} {'BL SignAcc':>12} {'Improvement':>12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for lt in LEAD_TIMES:\n",
    "    r = results[lt]\n",
    "    imp = r['ml_sign_acc'] - r['bl_sign_acc']\n",
    "    print(f\"LT={lt:2d} min  {r['ml_sign_acc']:>11.1f}% {r['bl_sign_acc']:>11.1f}% {imp:>+11.1f}%\")\n",
    "\n",
    "avg_ml_sign = np.mean([results[lt]['ml_sign_acc'] for lt in LEAD_TIMES])\n",
    "avg_bl_sign = np.mean([results[lt]['bl_sign_acc'] for lt in LEAD_TIMES])\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Average':<10} {avg_ml_sign:>11.1f}% {avg_bl_sign:>11.1f}% {avg_ml_sign - avg_bl_sign:>+11.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. SignAccQH - Combined Sign Accuracy per Quarter Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_signaccqh(predictions_dict):\n",
    "    \"\"\"Calculate sign accuracy per QH (>=3/5 correct signs).\"\"\"\n",
    "    qh_errors = []\n",
    "    \n",
    "    for st, preds in predictions_dict.items():\n",
    "        if len(preds) == 5:  # All 5 lead times present\n",
    "            errors = sum(1 for y, p, _ in preds if (y >= 0) != (p >= 0))\n",
    "            qh_errors.append(errors)\n",
    "    \n",
    "    return qh_errors\n",
    "\n",
    "ml_errors = calculate_signaccqh(predictions_store['ml'])\n",
    "bl_errors = calculate_signaccqh(predictions_store['baseline'])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SIGN ERROR DISTRIBUTION PER QH\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Errors':<10} {'ML Model':>12} {'Baseline':>12} {'Difference':>12}  Interpretation\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "interpretations = {\n",
    "    0: 'Perfect (5/5)',\n",
    "    1: 'Excellent (4/5)',\n",
    "    2: 'Good (3/5)',\n",
    "    3: 'Poor (2/5)',\n",
    "    4: 'Bad (1/5)',\n",
    "    5: 'Worst (0/5)'\n",
    "}\n",
    "\n",
    "for e in range(6):\n",
    "    ml_pct = 100 * sum(1 for x in ml_errors if x == e) / len(ml_errors)\n",
    "    bl_pct = 100 * sum(1 for x in bl_errors if x == e) / len(bl_errors)\n",
    "    diff = ml_pct - bl_pct\n",
    "    marker = '*' if e <= 2 else ''\n",
    "    print(f\"{e:<10} {ml_pct:>11.1f}% {bl_pct:>11.1f}% {diff:>+11.1f}%  {interpretations[e]} {marker}\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "ml_signaccqh = 100 * sum(1 for x in ml_errors if x <= 2) / len(ml_errors)\n",
    "bl_signaccqh = 100 * sum(1 for x in bl_errors if x <= 2) / len(bl_errors)\n",
    "print(f\"{'SignAccQH':<10} {ml_signaccqh:>11.1f}% {bl_signaccqh:>11.1f}% {ml_signaccqh - bl_signaccqh:>+11.1f}%  (<=2 errors)\")\n",
    "print(f\"\\nTotal QH periods: {len(ml_errors)}\")\n",
    "print(\"\\n* = Contributes to SignAccQH success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "for lt, model in models.items():\n",
    "    model_path = MODELS_PATH / f'model_production_lt{lt}.joblib'\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"Saved: {model_path}\")\n",
    "\n",
    "# Save configuration\n",
    "config = {\n",
    "    'features': FEATURES,\n",
    "    'model_params': MODEL_PARAMS,\n",
    "    'lead_times': LEAD_TIMES\n",
    "}\n",
    "config_path = MODELS_PATH / 'model_config.joblib'\n",
    "joblib.dump(config, config_path)\n",
    "print(f\"Saved: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PRODUCTION MODEL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n** Model Configuration **\")\n",
    "print(f\"Algorithm: HistGradientBoostingRegressor\")\n",
    "print(f\"Lead Times: {LEAD_TIMES}\")\n",
    "print(f\"Total Features: {sum(len(f) for f in FEATURES.values())} (across all models)\")\n",
    "\n",
    "print(\"\\n** Features per Lead Time **\")\n",
    "for lt in LEAD_TIMES:\n",
    "    print(f\"  LT={lt:2d}: {FEATURES[lt]}\")\n",
    "\n",
    "print(\"\\n** Performance vs Baseline **\")\n",
    "print(f\"  Average MAE Improvement: {avg_imp:+.1f}%\")\n",
    "print(f\"  Average SignAcc Improvement: {avg_ml_sign - avg_bl_sign:+.1f}%\")\n",
    "print(f\"  SignAccQH: {ml_signaccqh:.1f}% (vs {bl_signaccqh:.1f}% baseline, {ml_signaccqh - bl_signaccqh:+.1f}%)\")\n",
    "print(f\"  Perfect QH Predictions (0 errors): {100 * sum(1 for x in ml_errors if x == 0) / len(ml_errors):.1f}%\")\n",
    "\n",
    "print(\"\\n** Key Results **\")\n",
    "print(f\"  - ML model achieves {ml_signaccqh:.1f}% SignAccQH (>=3/5 correct signs per QH)\")\n",
    "print(f\"  - {100 * sum(1 for x in ml_errors if x == 0) / len(ml_errors):.1f}% of QH have perfect sign prediction (all 5 lead times correct)\")\n",
    "print(f\"  - Only {100 * sum(1 for x in ml_errors if x >= 3) / len(ml_errors):.1f}% of QH have poor predictions (<=2/5 correct)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Models saved to:\", MODELS_PATH)\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
